library(GradDesc)
library(GradDesc)
?gradient
?gradient
library(GradDesc)
?gradient
library(GradDesc)
?gradient
?fit_online_batch_Final
library(GradDesc)
?fit_online_batch_Final
?gradient
?fit
gradient_xy(data_ex, coef_ex)
#' The gradient of a function f for the parallelisation, denoted as ∇f, is the collection of all its partial derivatives into a vector
#'
#' @param data dataframe required to perform the gradient descent
#' @param theta Coefficient matrix for logistic regression
#'
#' @export
#' @return Returns a gradient vector
#'
#' @examples
#' gradient_xy(data_ex, coef_ex)
gradient_xy <- function(data, theta) {
y <- as.matrix(data[,1])
x <- as.matrix(data[,2:ncol(data)])
sig <- sigmoid(x%*%theta)
gradient <- (t(x) %*% (sig-y)) / nrow(y)
return(gradient)
}
#' Sigmoid Function
#'
#' The sigmoid function is also called a squashing function as its domain is the set of all real numbers, and its range is (0, 1)
#'
#' @param x matrix of explanatory variables to predict
#'
#' @export
#' @return Returns a vector of positive / negative class membership probabilities
#' @examples
#' sigmoid(x)
sigmoid <- function(x){
return(1/(1+exp(-x)))
}
#'
#' @param x Matrix of explanatory variables to predict
#' @param y Target Variable Matrix
#' @param theta Coefficient matrix for logistic regression
#'
#' @export
#' @return Returns a gradient vector
#'
#' @examples
#' gradient(x, y, coef)
gradient <- function(x, y, theta) {
sig <- sigmoid(x%*%theta)
gradient <- (t(x) %*% (sig-y)) / nrow(y)
return(gradient)
}
library(readxl)
library(parallel)
library(foreach)
library(doParallel)
# Sigmoid function
sigmoid <-
function(x){
return(1/(1+exp(-x)))
}
n = nrow(data)
blocs <- split(data,1+(1:n)%%2)
#print(class(blocs[[1]]))
# Gradient function
gradient_xy <- function(data, theta) {
y <- as.matrix(data[,1])
x <- as.matrix(data[,2:ncol(data)])
sig <- sigmoid(x%*%theta)
gradient <- (t(x) %*% (sig-y)) / nrow(y)
return(gradient)
}
# Gradient function
gradient <- function(x,y, theta) {
sig <- sigmoid(x%*%theta)
gradient <- (t(x) %*% (sig-y)) / nrow(y)
return(gradient)
}
# Loss function
cout = function(x,y,theta){
sig <- sigmoid(x%*%theta)
n <- nrow(y)
#cost = ((-y*log(sig))-((1-y)*log(1-sig)))/n #F
cost = (1 / n) * sum((-y * log(sig)) - ((1 - y) * log(1 - sig))) #R
#cost = (t(-y)%*%log(sig)-t(1-y)%*%log(1-sig))/n #S
return (cost)
}
data <- read_excel("C:/Users/doron/Documents/M2 SISE/Programmation stat sous R/Projet//breast.xlsx")
fit <- function(formula, data, mode, batch_size, ncores=0,coef,max_iter=10000,learningrate=0.1,tol=1e-4){
if (max_iter <= 0){
stop("'max_iter' must be greater than zero")
}
# Création de l'instance
instance <- list()
data = model.frame(formula, data = data)
modele = formula
x = as.matrix(data[,2:(ncol(data))])
x = as.matrix(apply(x, 2, scale))
b = matrix(1,nrow=nrow(x),ncol = 1)
x = as.matrix(cbind(b,x))
coef = matrix(coef,nrow=ncol(x),ncol = 1)
y = as.matrix(data[,1])
y = as.matrix(ifelse(y==data[1,1], 1,0))
cible <- data[1,1]
# Démarrage des moteurs (workers)
if (ncores!=0){
clust <- parallel::makeCluster(ncores)
}
iter <- 0
converge <- FALSE
loss_history = c()
while ((iter < max_iter) && (converge == FALSE)){
if (mode == "batch"){
if (ncores==0){
# Predict value for the model
c = cout(x,y,coef)
loss_history = c(loss_history,c)
z <- gradient(x, y, coef)
new_coef = coef-(learningrate*z)
}else{
data = as.data.frame(cbind(y,x))
n = nrow(data)
# Partition en blocs des données
blocs <- split(data,1+(1:n)%%ncores)
#print(class(blocs))
# ParSapply
clusterExport(cl=clust, varlist=c("sigmoid", "gradient","gradient_xy", "cout"))
res <- parallel::parSapply(clust,X=blocs,FUN=gradient_xy,theta=coef)
mean_coef=c()
for (i in 1:nrow(res)){
mean_coef = c(mean_coef,mean(res[i,]))
}
mean_coef <- as.matrix(mean_coef)
new_coef = coef-(learningrate*mean_coef)
blocs_1 <- as.matrix(blocs[[1]])
y_blocs <- as.matrix(blocs_1[,1])
x_blocs <- as.matrix(blocs_1[,2:ncol(blocs_1)])
#print(x_blocs)
# Predict value for the model
c = cout(x_blocs,y_blocs,mean_coef)
#print(loss_history)
loss_history = c(loss_history,c)
}
# Verification
if (sum(abs(new_coef-coef)) < tol){
converge <- TRUE
}
coef = new_coef
plot(loss_history, type='l')
}
if (mode=="mini-batch"){
# Tirage aléatoire
data = cbind(y,x)
rows <- sample(nrow(data))  # Melanger les indices du dataframe data
data <- data[rows,] # Utiliser ces indices pour reorganiser le dataframe
# Get the x and y batch
x_batch = matrix(data[1:batch_size,2:ncol(data)], nrow=batch_size, ncol=ncol(data)-1)
y_batch = as.matrix(data[1:batch_size,1])
# Loss function
c <- cout(x_batch,y_batch,coef)
loss_history <- c(loss_history,c)
# Gradient function
z <- gradient(x_batch, y_batch, coef)
new_coef = coef - learningrate*z
# Verification
if (sum(abs(new_coef-coef)) < tol){
converge <- TRUE
}
coef = new_coef
# Verification of the loss function
plot(loss_history, type="l")
}
if (mode=="online"){
# Tirage aléatoire
data = cbind(y,x)
rows <- sample(nrow(data))  # Melanger les indices du dataframe data
data <- data[rows,] # Utiliser ces indices pour reorganiser le dataframe
n=nrow(data)
for (i in 1:n){
# Get the x and y batch
x_batch = matrix(data[i,2:ncol(data)], nrow=1, ncol=ncol(data)-1)
y_batch = matrix(data[i,1])
# Loss function
c <- cout(x_batch,y_batch,coef)
loss_history <- c(loss_history,c)
# Gradient function
z <- gradient(x_batch, y_batch, coef)
new_coef = coef - learningrate*z
}
# Verification
if (sum(abs(new_coef-coef)) < tol){
converge <- TRUE
}
coef = new_coef
# Verification of the loss function
plot(loss_history, type="l")
}
# Next iteration
iter <- iter +1
}
# Eteindre les moteurs
if (ncores!=0){
parallel::stopCluster(clust)
}
instance$cible <- cible
instance$formula <- modele
instance$coefficients <- coef
instance$loss_history <- loss_history
instance$nb_iter_while <- iter
class(instance) <- "fit"
# Renvoyer le résultat
return(instance)
}
print(system.time(LogisticRegression <- fit(formula=default~.,coef=0.5,mode="batch",batch_size=1,data=data)))
print(system.time(LogisticRegression <- fit(formula=classe~.,coef=0.5,mode="batch",batch_size=1,data=data)))
#' @param coef Fixed number for creating the base vector
#' @param max_iter Max number of iterations not to be exceeded
#' @param learningrate Learning rate which determines the step of the descent
#' @param tol Tolerance threshold for which convergence is said to be "finished"
#'
#' @import parallel
#'
#' @return
#' @export
#'
fit <- function(formula, data, mode, batch_size, ncores=0,coef,max_iter=100,learningrate=0.1,tol=1e-4){
if ((mode != "batch") && (mode != "mini-batch") && (mode != "online")){
stop("mode incorrect")
}
if ((mode == "online") && (batch_size !=1)){
stop("batch size incorrect pour ce mode")
}
if (learningrate <= 0){
stop("learning rate incorrect")
}
if (max_iter <= 0){
stop("max_iter incorrect")
}
# Création de l'instance
instance <- list()
data = model.frame(formula, data = data)
modele = formula
x = as.matrix(data[,2:(ncol(data))])
x = as.matrix(apply(x, 2, scale))
b = matrix(1,nrow=nrow(x),ncol = 1)
x = as.matrix(cbind(b,x))
coef = matrix(coef,nrow=ncol(x),ncol = 1)
y = as.matrix(data[,1])
y = as.matrix(ifelse(y==data[1,1], 1,0))
cible = data[1,1]
# Démarrage des moteurs (workers)
if (ncores!=0){
clust = parallel::makeCluster(ncores)
}
iter = 0
converge = FALSE
loss_history = c()
while ((iter < max_iter) && (converge == FALSE)){
if (mode == "batch"){
if (ncores==0){
# Predict value for the model
c = cout(x,y,coef)
loss_history = c(loss_history,c)
z = gradient(x, y, coef)
new_coef = coef-(learningrate*z)
}else{
data = as.data.frame(cbind(y,x))
n = nrow(data)
# Partition en blocs des données
blocs = split(data,1+(1:n)%%ncores)
#print(class(blocs))
# ParSapply
clusterExport(cl=clust, varlist=c("sigmoid", "gradient","gradient_xy", "cout"))
res = parallel::parSapply(clust,X=blocs,FUN=gradient_xy,theta=coef)
mean_coef=c()
for (i in 1:nrow(res)){
mean_coef = c(mean_coef,mean(res[i,]))
}
mean_coef = as.matrix(mean_coef)
new_coef = coef-(learningrate*mean_coef)
blocs_1 = as.matrix(blocs[[1]])
y_blocs = as.matrix(blocs_1[,1])
x_blocs = as.matrix(blocs_1[,2:ncol(blocs_1)])
#print(x_blocs)
# Predict value for the model
c = cout(x_blocs,y_blocs,mean_coef)
#print(loss_history)
loss_history = c(loss_history,c)
}
# Verification
if (sum(abs(new_coef-coef)) < tol){
converge = TRUE
}
coef = new_coef
plot(loss_history, type='l',main = "Evolution of the cost function")
}
if (mode=="mini-batch"){
# Tirage aléatoire
data = cbind(y,x)
rows = sample(nrow(data))  # Melanger les indices du dataframe data
data = data[rows,] # Utiliser ces indices pour reorganiser le dataframe
# Get the x and y batch
x_batch = matrix(data[1:batch_size,2:ncol(data)], nrow=batch_size, ncol=ncol(data)-1)
y_batch = as.matrix(data[1:batch_size,1])
# Loss function
c = cout(x_batch,y_batch,coef)
loss_history = c(loss_history,c)
# Gradient function
z = gradient(x_batch, y_batch, coef)
new_coef = coef - learningrate*z
# Verification
if (sum(abs(new_coef-coef)) < tol){
converge = TRUE
}
coef = new_coef
# Verification of the loss function
plot(loss_history, type="l",main = "Evolution of the cost function")
}
if (mode=="online"){
# Tirage aléatoire
data = cbind(y,x)
rows = sample(nrow(data))  # Melanger les indices du dataframe data
data = data[rows,] # Utiliser ces indices pour reorganiser le dataframe
n=nrow(data)
for (i in 1:n){
# Get the x and y batch
x_batch = matrix(data[i,2:ncol(data)], nrow=1, ncol=ncol(data)-1)
y_batch = matrix(data[i,1])
# Loss function
c = cout(x_batch,y_batch,coef)
loss_history = c(loss_history,c)
# Gradient function
z = gradient(x_batch, y_batch, coef)
new_coef = coef - learningrate*z
}
# Verification
if (sum(abs(new_coef-coef)) < tol){
converge = TRUE
}
coef = new_coef
# Verification of the loss function
plot(loss_history, type="l",main = "Evolution of the cost function")
}
# Next iteration
iter = iter +1
}
# Eteindre les moteurs
if (ncores!=0){
parallel::stopCluster(clust)
}
instance$cible = cible
instance$index = colnames(data)
instance$formula = modele
instance$coefficients = coef
instance$loss_history = loss_history
instance$nb_iter_while = iter
class(instance) = "fit"
# Renvoyer le résultat
return(instance)
}
print(system.time(LogisticRegression <- fit(formula=default~.,coef=0.5,mode="batch",batch_size=1,data=data)))
print(system.time(LogisticRegression <- fit(formula=classe~.,coef=0.5,mode="batch",batch_size=1,data=data)))
#' prediction Binary or probabilities prediction
#'
#' @param objet the S3 object that the predict function returns
#' @param newdata dataframe explanatory variables on which to apply the prediction
#' @param type Output type "Posterior" (For predicted classes) or "class" (for the probability of class memberships)
#'
#'
#' @return An instance with the probability of class memberships, or predictions
#' @export
#'
predict = function(objet, newdata ,type){
instance = list()
instance$pred = c()
data = newdata
coef = objet$coefficients
index = objet$index
test = index != colnames(data[,2:(ncol(data))])
if(FALSE %in% test){
stop("Jeu de donn?es incorrect")
}
data = as.matrix(apply(data, 2, scale))
b = matrix(1,nrow=nrow(data),ncol = 1)
data = as.matrix(cbind(b,data))
sig = sigmoid(data %*% as.matrix(coef))
if(type=='posterior'){
instance$classe = sig
}else if(type=="class"){
instance$classe = sig
for(i in sig){
if(i >0.5){
instance$pred = c(instance$pred,1)
} else {
instance$pred = c(instance$pred,0)
}
}
}
class(instance) = "prediction"
return(instance)
}
library(caret) #this package has the createDataPartition function
set.seed(123) #randomization`
#creating indices
trainIndex <- createDataPartition(data$classe,p=0.75,list=FALSE)
#splitting data into training/testing data using the trainIndex object
data_TRAIN <- data[trainIndex,] #training data (75% of data)
data_TEST <- data[-trainIndex,] #testing data (25% of data)
X_Test = data_TEST[,1:9]
Y_Test = data_TEST[,10]
print(system.time(LogisticRegression <- fit(formula=classe~.,coef=0.5,mode="batch",batch_size=1,data=data_TRAIN)))
pred = predict(LogisticRegression,X_Test,type = "class")
